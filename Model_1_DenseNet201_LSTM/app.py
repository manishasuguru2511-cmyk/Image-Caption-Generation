import streamlit as st
import numpy as np
import pickle
import os
from PIL import Image
import tensorflow as tf
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.applications.densenet import DenseNet201, preprocess_input
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import Sequence
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer
from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional
from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from textwrap import wrap

# Load model, tokenizer, and constants
caption_model = load_model('model.keras')
tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))
max_length = 34  # Replace this with the actual value you used
vocab_size = len(tokenizer.word_index) + 1

# Load DenseNet201 for feature extraction
def get_feature_extractor():
    base_model = DenseNet201(weights='imagenet', include_top=False, pooling='avg')
    return base_model

feature_extractor = get_feature_extractor()

# Helper: index to word
def idx_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

# Generate caption for uploaded image
def generate_caption(image, model, tokenizer, max_length, feature_extractor):
    # Preprocess the image
    image = image.resize((224, 224))
    image_array = img_to_array(image) / 255.0
    image_array = np.expand_dims(image_array, axis=0)
    image_array = preprocess_input(image_array)

    # Extract features
    feature = feature_extractor.predict(image_array)
    
    # Predict caption
    in_text = 'startseq'
    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        y_pred = model.predict([feature, sequence], verbose=0)
        y_pred = np.argmax(y_pred)
        word = idx_to_word(y_pred, tokenizer)
        if word is None:
            break
        in_text += ' ' + word
        if word == 'endseq':
            break

    return in_text.replace('startseq', '').replace('endseq', '').strip()

# ---------------- Streamlit UI ----------------
st.set_page_config(page_title="🖼️ Image Captioning", layout="centered")
st.title("🖼️ Image Caption Generator")
st.markdown("Upload an image to get a caption generated by a CNN + LSTM model.")

uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Uploaded Image", use_column_width=True)

    with st.spinner('Generating caption...'):
        caption = generate_caption(image, caption_model, tokenizer, max_length, feature_extractor)

    st.success("Generated Caption:")
    st.markdown(f"> {caption}")
